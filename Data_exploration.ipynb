{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d5e1ef8",
   "metadata": {},
   "source": [
    "### Step 1. Importing and preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b581dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # helps to read images froma a directory\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # loading and processing images for Deep Learning\n",
    "from tensorflow.keras.models import Sequential # to build a Deep Learning model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout # layers for the Deep Learning model\n",
    "from tensorflow.keras.optimizers import Adam # optimizer for training the model\n",
    "from sklearn.metrics import classification_report, confusion_matrix # evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b70becbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/chest_xray/train'\n",
    "categories = ['NORMAL', 'PNEUMONIA']\n",
    "filepaths = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49e88044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each category and get file paths and labels\n",
    "for category in categories:\n",
    "    folder = os.path.join(train_dir, category)\n",
    "    for fname in os.listdir(folder):\n",
    "        filepaths.append(f\"{category}/{fname}\")\n",
    "        labels.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fb1f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Filename': filepaths, 'Label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2049681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaling pixel values (lower it) and setting aside validation data (80% training and 20% validation)\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "636cf96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4173 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Loading training data from dataframe\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=train_dir,\n",
    "    x_col='Filename',\n",
    "    y_col='Label',\n",
    "    subset='training',\n",
    "    batch_size=32, # processing 32 images at a time\n",
    "    seed=42, # starting point for random operations\n",
    "    shuffle=True, # shuffling data before the training\n",
    "    class_mode='binary', # binary classification (NORMAL vs PNEUMONIA)\n",
    "    target_size=(150, 150) # resizing images to 150x150 pixels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bbd73e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1043 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# validation data from dataframe\n",
    "valid_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=train_dir,\n",
    "    x_col='Filename',\n",
    "    y_col='Label',\n",
    "    subset='validation',\n",
    "    batch_size=32, # processing 32 images at a time\n",
    "    seed=42, # starting point for random operations\n",
    "    shuffle=True, # shuffling data before the training\n",
    "    class_mode='binary', # binary classification (NORMAL vs PNEUMONIA)\n",
    "    target_size=(150, 150) # resizing images to 150x150 pixels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c47be4",
   "metadata": {},
   "source": [
    "### Step 2. Building a CNN model (Convolution, ReLU, Pooling, Dense Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b57ee7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Repositories\\computer-vision\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "''' Building the CNN model\n",
    "    Sequential model adds one layer after another in sequence, like a pipeline. Each layer transforms the data in some way.\n",
    "    Conv2D layer applies convolution operation to extract features from images using filters/kernels.\n",
    "    MaxPooling2D layer reduces the spatial dimensions (width and height) of the feature maps, retaining important information while reducing computational load.\n",
    "    Flatten layer converts the 2D feature maps into a 1D vector, preparing it for the fully connected layers.\n",
    "    Dropout layer randomly sets a fraction of input units to 0 during training to prevent overfitting.\n",
    "    Dense layer is a fully connected layer where each neuron is connected to every neuron in the previous layer.\n",
    "    The final Dense layer with a sigmoid activation function outputs a probability score between 0 and 1 for binary classification (NORMAL vs PNEUMONIA).\n",
    "'''\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)), # finding the patterns in the images; 3 channels for RGB.\n",
    "    MaxPooling2D((2, 2)), \n",
    "    Conv2D(64, (3, 3), activation='relu'), # 64 filters to learn more complex features\n",
    "    MaxPooling2D((2, 2)), \n",
    "    Conv2D(128, (3, 3), activation='relu'), # 128 filters for even more complex features\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(), \n",
    "    Dropout(0.5), # prevent overfitting by randomly dropping 50% of the neurons during training\n",
    "    Dense(128, activation='relu'), # 128 neurons in this fully connected layer\n",
    "    Dense(1, activation='sigmoid') # 1 neuron for binary classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2f21b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36992</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36992</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,735,104</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36992\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36992\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m4,735,104\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,828,481</span> (18.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,828,481\u001b[0m (18.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,828,481</span> (18.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,828,481\u001b[0m (18.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" model compilation\n",
    "    Adam optimizer is an efficient optimization algorithm that adjusts the learning rate during training.\n",
    "    The output layer uses 'sigmoid' activation function suitable for binary classification tasks.\n",
    "    Binary crossentropy is the loss function used for binary classification tasks.\"\"\"\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e2775",
   "metadata": {},
   "source": [
    "![Output](assets/Output_1.png)\n",
    "\n",
    "Output gave 3 columns: Layer (type), Output Shape and Param #. \n",
    "1. **Layer (type)** - is a type of neural network. This represents the specific mathematical operation performed on the data at this step.\n",
    "\n",
    "    - **conv2d**: These are the \"filters\" scanning the X-ray for patterns like edges or cloudy areas.\n",
    "\n",
    "    - **max_pooling2d**: This simplifies the image, reducing the resolution to keep only the most important features.\n",
    "\n",
    "    - **dense**: This is the \"brain\" part of the model that takes all the patterns found and decides if they look like Pneumonia.\n",
    "\n",
    "2. **Output Shape** - This tells you the dimensions of the data after it passes through the layer. E.g. In the image, (None, 148, 148, 32) means:\n",
    "\n",
    "    - **None**: This is a placeholder for the Batch Size (how many images you process at once).\n",
    "\n",
    "    - **148, 148**: The new height and width of the image (it gets smaller after convolutions).\n",
    "\n",
    "    - **32**: The number of Filters (features) the layer is looking for.\n",
    "\n",
    "3. **Param #** - This is the number of \"weights\" or \"learned connections\" the model is adjusting during training. \n",
    "- (First row) Param # (896): \n",
    "    - Filter size is a standard 3x3 kernel: **3 * 3 = 9**\n",
    "    - Since X-rays are usually treated as RGB in these models, there are 3 channels (Red, Green, Blue): **9 * 3 = 27**)** \n",
    "    - Each filter has **1** bias weight: **27 + 1 = 28**\n",
    "    - Number of Filters: This layer has **32 filters**.\n",
    "    \n",
    "\n",
    "        $$\n",
    "        \\text{Parameters = (filter\\_height * filter\\_width * input\\_channels + 1) * number\\_of\\_filters}\n",
    "        $$\n",
    "\n",
    "        $$\n",
    "        \\text{28 (weights per filter) * 32 (filters) = 896 total parameters}\n",
    "        $$\n",
    "\n",
    "- (Second row) Output Shape --> (None, 72, 72, 32) - the size is reduced by half.\n",
    "- Param # (0) --> Unlike a convolutional layer, which has filters (weights) that the model must adjust to learn patterns, Pooling simply follows a rule. It looks at a small window of pixels (usually 2 x 2) and simply picks the largest value (Max) to pass to the next layer. Because the rule \"pick the highest number\" never changes, there is nothing for the model to \"train\" or store in its memory for this specific layer, that is why it is '0'.\n",
    "\n",
    "- (Third row) Param # (18,496):\n",
    "    - Filter size is a standard 3x3 kernel: **3 * 3 = 9**\n",
    "    - Since the previous layer had **32 filters**, so this layer receives 32 channels of data: **9 * 32 = 288**\n",
    "    - Each filter has **1** bias weight: **288 + 1 = 289**\n",
    "    - Number of Filters: This layer has **64 filters**.\n",
    "\n",
    "        $$\n",
    "        \\text{289 (weights per filter) * 64 (filters) = 18,496 total parameters}\n",
    "        $$\n",
    "\n",
    "- Notice the first dense layer has ``4,735,104`` parameters! This is where most of the \"learning\" happens.\n",
    "\n",
    "- Total parameters (18.42 MB) tells you the \"memory size\" of your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c69233e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 474ms/step - accuracy: 0.9239 - loss: 0.2061 - val_accuracy: 0.0000e+00 - val_loss: 1.2994\n",
      "Epoch 2/10\n",
      "\u001b[1m  1/130\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 209ms/step - accuracy: 0.9688 - loss: 0.0557"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Repositories\\computer-vision\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 142ms/step - accuracy: 0.9688 - loss: 0.0557 - val_accuracy: 0.0000e+00 - val_loss: 1.2985\n",
      "Epoch 3/10\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 442ms/step - accuracy: 0.9539 - loss: 0.1057 - val_accuracy: 0.5244 - val_loss: 1.1249\n",
      "Epoch 4/10\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 146ms/step - accuracy: 0.9688 - loss: 0.1655 - val_accuracy: 0.5322 - val_loss: 1.0951\n",
      "Epoch 5/10\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 433ms/step - accuracy: 0.9626 - loss: 0.0977 - val_accuracy: 0.5977 - val_loss: 0.9987\n",
      "Epoch 6/10\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 139ms/step - accuracy: 1.0000 - loss: 0.0744 - val_accuracy: 0.5918 - val_loss: 1.0015\n",
      "Epoch 7/10\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 458ms/step - accuracy: 0.9744 - loss: 0.0829 - val_accuracy: 0.7373 - val_loss: 0.9032\n",
      "Epoch 8/10\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 149ms/step - accuracy: 0.9375 - loss: 0.1971 - val_accuracy: 0.8203 - val_loss: 0.7919\n",
      "Epoch 9/10\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 462ms/step - accuracy: 0.9751 - loss: 0.0766 - val_accuracy: 0.5664 - val_loss: 1.2780\n",
      "Epoch 10/10\n",
      "\u001b[1m130/130\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 152ms/step - accuracy: 0.9375 - loss: 0.0838 - val_accuracy: 0.6279 - val_loss: 1.2064\n"
     ]
    }
   ],
   "source": [
    "''' Training the model:\n",
    "    steps_per_epoch: number of batches to process before declaring one epoch finished.\n",
    "    epochs: number of times the model will go through the entire training dataset.\n",
    "    validation_data: data on which to evaluate the loss and any model metrics at the end of each epoch.\n",
    "    validation_steps: number of batches to process from the validation data at the end of each epoch.\n",
    "'''\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=10, # number of times the model will go through the entire training dataset\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=valid_generator.samples // valid_generator.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30907550",
   "metadata": {},
   "source": [
    "### Testing the model on the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd9f4172",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'data/chest_xray/test'\n",
    "test_filepaths = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99fe06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    folder = os.path.join(test_dir, category)\n",
    "    for fname in os.listdir(folder):\n",
    "        test_filepaths.append(f\"{category}/{fname}\")\n",
    "        test_labels.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bbb57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'Filename': test_filepaths, 'Label': test_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0dc9bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c40ed6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Loading training data from dataframe\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory=test_dir,\n",
    "    x_col='Filename',\n",
    "    y_col='Label',\n",
    "    batch_size=32, # processing 32 images at a time\n",
    "    shuffle=False, # shuffling data before the training\n",
    "    class_mode='binary', # binary classification (NORMAL vs PNEUMONIA)\n",
    "    target_size=(150, 150) # resizing images to 150x150 pixels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b75eab",
   "metadata": {},
   "source": [
    "### Prediction step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5467d3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      NORMAL       1.00      0.12      0.21       234\n",
      "   PNEUMONIA       0.65      1.00      0.79       390\n",
      "\n",
      "    accuracy                           0.67       624\n",
      "   macro avg       0.83      0.56      0.50       624\n",
      "weighted avg       0.78      0.67      0.57       624\n",
      "\n",
      "[[ 28 206]\n",
      " [  0 390]]\n"
     ]
    }
   ],
   "source": [
    "pred_probs = model.predict(test_generator)\n",
    "preds = (pred_probs > 0.5).astype(int).flatten()\n",
    "true_labels = test_generator.classes\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "print(classification_report(true_labels, preds, target_names=class_labels))\n",
    "print(confusion_matrix(true_labels, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
